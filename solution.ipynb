{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from data_preprocessing_utils import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'HeadHunter_train.csv'\n",
    "TEST_DATA_PATH = 'HeadHunter_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "train = preprocess_data(train)\n",
    "test = preprocess_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"DeepPavlov/rubert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "spec_toks = ['[%s%d]' % (g,r) for g in ['A', 'B', 'C', 'D', 'E', 'F'] for r in range(1,6) ]\n",
    "\n",
    "tokenizer.add_tokens(spec_toks, special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing_utils import make_text_features_with_rating_tokens, collate_fn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, token_types = make_text_features_with_rating_tokens(train)\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = torch.LongTensor(mlb.fit_transform(train.target.apply(lambda x:x.split(',')))).float()\n",
    "dataset_train = TensorDataset(input_ids, attention_mask, token_types, labels)\n",
    "batch_size = 16\n",
    "train_size = int(0.95 * len(dataset_train))\n",
    "val_size = len(dataset_train) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset) ,batch_size = batch_size, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size = batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import get_optimizer_grouped_parameters, train_loop, model_eval, collate_fn_pred, get_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_config = {\n",
    "    'from_pretrained':\"DeepPavlov/rubert-base-cased\",\n",
    "    'cls_dim' : 768, \n",
    "    'n_classes' : 9, \n",
    "    'h_dim' : None,\n",
    "    'p': 0.5\n",
    "}\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_config = {\n",
    "    'bert_lr':{'bert_lr':5e-5, 'embed_lr':1e-6, 'n_layers': 9, 'lr_decay': 0.9, 'n_reinit':0},\n",
    "    'task_lr':  1e-4, \n",
    "    'epoch': 2, \n",
    "    'loss_fn': torch.nn.BCEWithLogitsLoss(), \n",
    "    'opt_param_fn' : get_optimizer_grouped_parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BertMeanMaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertMeanMaxPooling(**bert_config)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "t = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, train_dataloader, train_config, model_eval, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = make_text_features_with_rating_tokens(test)\n",
    "dataset_test = TensorDataset(input_ids, attention_mask)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size = batch_size, collate_fn = collate_fn_pred)\n",
    "pred = get_predict(model, test_dataloader, binary = False)\n",
    "save_submit(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
